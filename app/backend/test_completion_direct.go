//go:build ignore

package main

import (
	"context"
	"fmt"
	"time"

	"github.com/bareuptime/tms/internal/config"
	"github.com/bareuptime/tms/internal/db"
	"github.com/bareuptime/tms/internal/logger"
	"github.com/bareuptime/tms/internal/models"
	"github.com/bareuptime/tms/internal/repo"
	"github.com/bareuptime/tms/internal/service"
	"github.com/google/uuid"
)

func main() {
	logger.Info("=== Testing Job Completion Status Fix ===")
	logger.Info("")

	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		logger.Errorf("Failed to load config: %v", err)
		return
	}

	// Connect to database
	database, err := db.Connect(&cfg.Database)
	if err != nil {
		logger.Errorf("Failed to connect to database: %v", err)
		return
	}
	defer database.Close()

	// Create repositories and services
	knowledgeRepo := repo.NewKnowledgeRepository(database.DB)
	embeddingService := service.NewEmbeddingService(&cfg.Knowledge)
	webScraperService := service.NewWebScrapingService(knowledgeRepo, embeddingService, &cfg.Knowledge)

	// Use existing test tenant and project IDs
	tenantID := uuid.MustParse("550e8400-e29b-41d4-a716-446655440000")
	projectID := uuid.MustParse("550e8400-e29b-41d4-a716-446655440001")

	logger.Infof("üéØ Testing job completion with tenant: %s, project: %s", tenantID, projectID)
	logger.Infof("üîß Embedding service enabled: %v", embeddingService.IsEnabled())
	logger.Info("")

	// Create scraping request for a simple HTML page
	req := &models.CreateScrapingJobRequest{
		URL:      "https://httpbin.org/html",
		MaxDepth: 1,
	}

	logger.Infof("üöÄ Creating scraping job for URL: %s", req.URL)

	// Create the job using the service
	job, err := webScraperService.CreateScrapingJob(context.Background(), tenantID, projectID, req)
	if err != nil {
		logger.Errorf("‚ùå Failed to create scraping job: %v", err)
		return
	}

	logger.Infof("‚úÖ Created job: %s", job.ID)
	logger.Infof("üìä Initial status: %s", job.Status)

	// Monitor job status for completion
	logger.Info("‚è∞ Monitoring job status for completion...")

	maxWait := 90 * time.Second
	checkInterval := 3 * time.Second
	startTime := time.Now()
	lastStatus := ""

	for time.Since(startTime) < maxWait {
		// Get current job status
		currentJob, err := webScraperService.GetScrapingJob(context.Background(), job.ID)
		if err != nil {
			logger.Errorf("‚ùå Error getting job status: %v", err)
			time.Sleep(checkInterval)
			continue
		}

		elapsed := time.Since(startTime)

		// Only print if status changed
		if currentJob.Status != lastStatus {
			statusMsg := fmt.Sprintf("üìä [%s] Status changed: %s ‚Üí %s",
				elapsed.Round(time.Second),
				lastStatus,
				currentJob.Status)

			if currentJob.PagesScraped > 0 {
				statusMsg += fmt.Sprintf(", Pages: %d", currentJob.PagesScraped)
			}

			if currentJob.StartedAt != nil {
				statusMsg += fmt.Sprintf(", Started: %s", currentJob.StartedAt.Format("15:04:05"))
			}
			if currentJob.CompletedAt != nil {
				statusMsg += fmt.Sprintf(", Completed: %s", currentJob.CompletedAt.Format("15:04:05"))
			}
			if currentJob.ErrorMessage != nil {
				statusMsg += fmt.Sprintf(", Error: %s", *currentJob.ErrorMessage)
			}
			logger.Info(statusMsg)

			lastStatus = currentJob.Status
		}

		// Check if job is complete
		if currentJob.Status == models.JobStatusCompleted {
			logger.Info("üéâ SUCCESS: Job completed successfully!")

			// Verify completion details
			logger.Infof("‚úÖ Final status: %s", currentJob.Status)
			logger.Infof("‚úÖ Pages scraped: %d", currentJob.PagesScraped)
			logger.Infof("‚úÖ Started at: %v", currentJob.StartedAt)
			logger.Infof("‚úÖ Completed at: %v", currentJob.CompletedAt)

			if currentJob.StartedAt != nil && currentJob.CompletedAt != nil {
				duration := currentJob.CompletedAt.Sub(*currentJob.StartedAt)
				logger.Infof("‚úÖ Processing time: %s", duration.Round(time.Millisecond))
			}

			// Check scraped pages
			pages, err := webScraperService.GetJobPages(context.Background(), job.ID)
			if err != nil {
				logger.Errorf("‚ùå Error getting scraped pages: %v", err)
			} else {
				logger.Infof("üìÑ Scraped pages: %d", len(pages))
				for i, page := range pages {
					logger.Infof("   Page %d: %s (tokens: %d)", i+1, page.URL, page.TokenCount)
					if page.Title != nil {
						logger.Infof("           Title: %s", *page.Title)
					}
				}
			}

			logger.Info("üîß FIX VERIFICATION:")
			logger.Info("‚úÖ Job properly transitions to 'completed' status")
			logger.Info("‚úÖ CompletedAt timestamp is set")
			logger.Info("‚úÖ Pages are scraped and saved")
			logger.Info("‚úÖ Job completion works even with embedding service disabled")
			break
		} else if currentJob.Status == "failed" || currentJob.Status == "error" {
			logger.Infof("‚ùå Job failed with status: %s", currentJob.Status)
			if currentJob.ErrorMessage != nil {
				logger.Infof("‚ùå Error: %s", *currentJob.ErrorMessage)
			}

			logger.Info("üîß FIX VERIFICATION:")
			logger.Info("‚úÖ Job properly transitions to failure status")
			logger.Info("‚úÖ Error message is stored")
			break
		}

		time.Sleep(checkInterval)
	}

	if time.Since(startTime) >= maxWait {
		logger.Info("‚è∞ Test timeout reached")

		// Get final status
		finalJob, err := webScraperService.GetScrapingJob(context.Background(), job.ID)
		if err == nil {
			logger.Infof("üìä Final status: %s", finalJob.Status)
			logger.Infof("üìä Pages scraped: %d", finalJob.PagesScraped)
		}
	}

	logger.Info("=== Job Completion Status Fix Test Complete ===")
}
